{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of kitti_modify_loss_run_MonocularDepth_loss.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"0b79a8fafc9c438883598ab807f9742e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_b651f60f6d3a43659907e4eab45696ea","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_e792657230da4674a070b68eaf9a83a1","IPY_MODEL_59950f5a17da4bff8d83650b349fd2a0"]}},"b651f60f6d3a43659907e4eab45696ea":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e792657230da4674a070b68eaf9a83a1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_d5c6bb7a0e254454b68501d9cb2d826f","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":57365526,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":57365526,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_0d8449c52fe144179dab8be4d7925fac"}},"59950f5a17da4bff8d83650b349fd2a0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_c73018b4a13a4efb8e3dd5702f7dcbe1","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 54.7M/54.7M [00:00&lt;00:00, 84.0MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_5e22736ee103410a850f9f67b9803997"}},"d5c6bb7a0e254454b68501d9cb2d826f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"0d8449c52fe144179dab8be4d7925fac":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c73018b4a13a4efb8e3dd5702f7dcbe1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"5e22736ee103410a850f9f67b9803997":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E4zolWjaMiBv","executionInfo":{"status":"ok","timestamp":1627984988091,"user_tz":-480,"elapsed":110505,"user":{"displayName":"崔人文","photoUrl":"","userId":"05996306314003136969"}},"outputId":"267c0a62-63e0-4f1b-fdeb-20adf3209980"},"source":["# kitti dataset\n","!gdown --id '14P9-IjC63F-CNqbypmD1d2m4tJV7xVQw' --output KITTI_small_data.zip"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Downloading...\n","From: https://drive.google.com/uc?id=14P9-IjC63F-CNqbypmD1d2m4tJV7xVQw\n","To: /content/KITTI_small_data.zip\n","4.98GB [00:46, 108MB/s] \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"uoykgDEgfGBq"},"source":["import zipfile\n","from zipfile import ZipFile\n","with zipfile.ZipFile(\"KITTI_small_data.zip\",\"r\") as zip_ref:\n","    zip_ref.extractall(\"targetdir\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pX1sdm8XfOzS"},"source":["Data"]},{"cell_type":"code","metadata":{"id":"LEtRUiWvfHWs"},"source":["import numpy as np\n","import torch\n","import random\n","\n","from torch.utils.data import Dataset, DataLoader\n","from PIL import Image\n","\n","\n","class KITTI_TrainAugmentDataset(Dataset):\n","    def __init__(self, files, inpainted_directory, transform=None):\n","        self.files = files\n","        self.inpainted_directory = inpainted_directory\n","\n","    def __getitem__(self, idx):\n","        # sample is a list containing names of a RGB image and corresponding depth image\n","        directory, image_ID = self.files[idx]\n","        # PIL.Image.open() Opens and identifies the given image file.\n","        image = Image.open(os.path.join(self.inpainted_directory,directory,\"rgb_image03_\"+image_ID)) \n","        depth = Image.open(os.path.join(self.inpainted_directory,directory,\"d_image03_\"+image_ID)) \n","        # Augmentation: random horizontal flip\n","        if random.random() < 0.5:\n","            image = image.transpose(Image.FLIP_LEFT_RIGHT)\n","            depth = depth.transpose(Image.FLIP_LEFT_RIGHT)\n","        # resize the depth ground truth from (1280,384) to (640,192) that is consist with the size of the predicted output.\n","        image = image.resize((640,192))\n","        depth = depth.resize((320,96))\n","        \n","        image = torch.clamp(torch.from_numpy(np.array(image).reshape(192,640,3)).float()/255,0,1).permute(2, 0, 1)\n","        depth = torch.clamp(torch.from_numpy(np.array(depth).reshape(96,320,1)).float()/256-5,0,80).permute(2, 0, 1)\n","        sample = {'image': image, 'depth': depth}\n","        return sample\n","\n","    def __len__(self):\n","        return len(self.files)\n","\n","\n","class KITTI_TestDataset(Dataset):\n","    def __init__(self, files, val_inpainted_directory, transform=None):\n","        self.files = files\n","        self.inpainted_directory = val_inpainted_directory\n","\n","    def __getitem__(self, idx):\n","        # sample is a list containing names of a RGB image and corresponding depth image\n","        directory, image_ID = self.files[idx]\n","        # PIL.Image.open() Opens and identifies the given image file.\n","        image = Image.open(os.path.join(self.inpainted_directory,directory,\"rgb_image03_\"+image_ID)) \n","        depth = Image.open(os.path.join(self.inpainted_directory,directory,\"d_image03_\"+image_ID))  \n","        # resize the depth ground truth from (1280,384) to (640,192) that is consist with the size of the predicted output.\n","        image = image.resize((640,192))\n","        depth = depth.resize((320,96))\n","        \n","        image = torch.clamp(torch.from_numpy(np.array(image).reshape(192,640,3)).float()/255,0,1).permute(2, 0, 1)\n","        depth = torch.clamp(torch.from_numpy(np.array(depth).reshape(96,320,1)).float()/256-5,0,80).permute(2, 0, 1)\n","        sample = {'image': image, 'depth': depth}\n","        return sample\n","\n","    def __len__(self):\n","        return len(self.files)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Z-CsXSw1fRCz"},"source":["loss"]},{"cell_type":"code","metadata":{"id":"H9iclK4Bldw_"},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from math import exp\n","\n","def L1loss(y_pred, y_true):\n","    l1_criterion = nn.L1Loss()\n","    l_depth = l1_criterion(y_pred, y_true)\n","    return l_depth\n","\n","def lossgradient(y_pred, y_true):\n","    # y_true gradient step=1\n","    y_pred_left = y_pred\n","    y_pred_right = F.pad(y_pred, [0, 1, 0, 0])[:, :, :, 1:]\n","    y_pred_top = y_pred\n","    y_pred_bottom = F.pad(y_pred, [0, 0, 0, 1])[:, :, 1:, :]\n","    # dx, dy = torch.abs(right - left), torch.abs(bottom - top)\n","    dx_pred, dy_pred = y_pred_right - y_pred_left, y_pred_bottom - y_pred_top \n","    dx_pred[:, :, :, -1] = 0\n","    dy_pred[:, :, -1, :] = 0\n","\n","    # y_true gradient step=1\n","    y_true_left = y_true\n","    y_true_right = F.pad(y_true, [0, 1, 0, 0])[:, :, :, 1:]\n","    y_true_top = y_true\n","    y_true_bottom = F.pad(y_true, [0, 0, 0, 1])[:, :, 1:, :]\n","    # dx, dy = torch.abs(right - left), torch.abs(bottom - top)\n","    dx_true, dy_true = y_true_right - y_true_left, y_true_bottom - y_true_top \n","    # dx will always have zeros in the last column, right-left\n","    # dy will always have zeros in the last row,    bottom-top\n","    dx_true[:, :, :, -1] = 0\n","    dy_true[:, :, -1, :] = 0\n","    \n","    l_edges = torch.mean(torch.abs(dy_pred - dy_true) + torch.abs(dx_pred - dx_true))\n","    return l_edges\n","\n","def gaussian(window_size, sigma):\n","    gauss = torch.Tensor([exp(-(x - window_size//2)**2/float(2*sigma**2)) for x in range(window_size)])\n","    return gauss/gauss.sum()\n","\n","def create_window(window_size, channel=1):\n","    _1D_window = gaussian(window_size, 1.5).unsqueeze(1)\n","    _2D_window = _1D_window.mm(_1D_window.t()).float().unsqueeze(0).unsqueeze(0)\n","    window = _2D_window.expand(channel, 1, window_size, window_size).contiguous()\n","    return window\n","\n","def ssim(img1, img2, maxDepthVal, window_size=11, window=None, size_average=True, full=False):\n","    L = maxDepthVal\n","\n","    padd = 0\n","    (_, channel, height, width) = img1.size()\n","    if window is None:\n","        real_size = min(window_size, height, width)\n","        window = create_window(real_size, channel=channel).to(img1.device)\n","\n","    mu1 = F.conv2d(img1, window, padding=padd, groups=channel)\n","    mu2 = F.conv2d(img2, window, padding=padd, groups=channel)\n","\n","    mu1_sq = mu1.pow(2)\n","    mu2_sq = mu2.pow(2)\n","    mu1_mu2 = mu1 * mu2\n","\n","    sigma1_sq = F.conv2d(img1 * img1, window, padding=padd, groups=channel) - mu1_sq\n","    sigma2_sq = F.conv2d(img2 * img2, window, padding=padd, groups=channel) - mu2_sq\n","    sigma12 = F.conv2d(img1 * img2, window, padding=padd, groups=channel) - mu1_mu2\n","\n","    C1 = (0.01 * L) ** 2\n","    C2 = (0.03 * L) ** 2\n","\n","    v1 = 2.0 * sigma12 + C2\n","    v2 = sigma1_sq + sigma2_sq + C2\n","    cs = torch.mean(v1 / v2)  # contrast sensitivity\n","\n","    ssim_map = ((2 * mu1_mu2 + C1) * v1) / ((mu1_sq + mu2_sq + C1) * v2)\n","\n","    if size_average:\n","        ret = ssim_map.mean()\n","    else:\n","        ret = ssim_map.mean(1).mean(1).mean(1)\n","\n","    if full:\n","        return ret, cs\n","\n","    return ret\n","\n","def depth_loss(y_pred, y_true):\n","    l_depth = L1loss(y_pred, y_true)\n","    l_edges = lossgradient(y_pred, y_true)\n","    l_ssim = torch.clamp((1 - ssim(y_pred, y_true, maxDepthVal=1000.0/10.0)) * 0.5, 0, 1)\n","    \n","    w1 = 1.0\n","    w2 = 1.0\n","    w3 = 1.0\n","    loss = ((w1 * l_depth) + (w2 * l_edges) + (w3 * l_ssim))\n","    return loss"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yJUTCOLhocUZ"},"source":["loss_modify"]},{"cell_type":"code","metadata":{"id":"ji3hbLwDfTk3"},"source":["# import torch\n","# import torch.nn.functional as func\n","# from math import exp\n","\n","# #Ldepth(y, ŷ)\n","# def l1_criterion(y_pred,y_true):\n","#   #compute point-wise L1 loss\n","#   l_depth = torch.mean(torch.abs(y_pred - y_true))\n","#   return l_depth\n","\n","# #Lgrad(y, ŷ)\n","# def image_gradients(image):\n","#   #compute image gradient loss\n","#   left = image\n","#   right = func.pad(image,[0, 1, 0, 0])[:, :, :, 1:]\n","#   top = image\n","#   bottom = func.pad(image, [0, 0, 0, 1])[:, :, 1:, :]\n","\n","#   dx = right - left\n","#   dy = bottom - top\n","#   dx[:, :, :, -1] = 0\n","#   dy[:, :, -1, :] = 0\n","\n","#   return dx,dy\n","\n","# #LSSIM (y, ŷ)\n","# #Structural similarity index is a method for predicting similarity of two images\n","# #An image quality metric that assesses the visual impact of three characteristics of an image: luminance, contrast and structure.\n","# def gaussian(window_size, sigma):\n","#   # Calculate the one-dimensional Gaussian distribution vector\n","#   def gauss(point):\n","#     return -(point - (window_size // 2))**2 / float(2 * sigma**2)\n","#   gauss = torch.Tensor([exp(gauss(point)) for point in range(window_size)])\n","#   return gauss / gauss.sum()\n","\n","# def create_window(window_size, channel=1):\n","#   # Create a Gaussian kernel, obtained by matrix multiplication of two one-dimensional Gaussian distribution vectors\n","#   gaussian_kernel1d = gaussian(window_size, 1.5).unsqueeze(1)\n","#   gaussian_kernel2d = gaussian_kernel1d.mm(gaussian_kernel1d.t()).float().unsqueeze(0).unsqueeze(0)\n","#   window = gaussian_kernel2d.expand(channel, 1, window_size, window_size).contiguous()\n","#   return window\n","\n","# def ssim(y_pred, y_true, data_range=None, window=None, size_average=True):\n","#   #If the data is not provided use max and min value from image to calculate data range\n","#   if data_range is None:\n","#     if torch.max(y_pred) > 128:\n","#       max_point = 255\n","#     else:\n","#       max_point = 1\n","\n","#     if torch.min(y_pred) < -0.5:\n","#       min_point = -1\n","#     else:\n","#       min_point = 0\n","#     data_range = max_point - min_point\n","\n","\n","#     #get parameter from image\n","#     (_, channel, height, width) = y_pred.size()\n","#     #window_size is 11 by default\n","#     default_window_size=11\n","\n","#     if window is None:\n","#         realWindowsize = min(default_window_size, height, width)\n","#         window = create_window(realWindowsize, channel=channel).to(y_pred.device)\n","\n","#     #The formula Var(X)=E[X^2]-E[X]^2, cov(X,Y)=E[XY]-E[X]E[Y] is used when calculating variance and covariance .    \n","#     #mu_x the average of x\n","#     mu_x = func.conv2d(y_pred, window, padding=0, groups=channel)\n","#     #mu_y the average of y\n","#     mu_y = func.conv2d(y_true, window, padding=0, groups=channel)\n","#     #sigma_xy the covariance of x and y\n","#     Sigma_xy = func.conv2d(y_pred * y_true, window, padding=0, groups=channel) - (mu_x * mu_x) * (mu_y * mu_y)\n","\n","#     #K1 = 0.01 and k2 = 0.03 by default\n","#     K1 = 0.01\n","#     K2 = 0.03\n","#     #C1 and C2 two variables to stabilize the division with weak denominator\n","#     ##L is the dynamic range of the pixel-values which either provided by user or calculate from before\n","#     C1 = (K1 * data_range) ** 2\n","#     C2 = (K2 * data_range) ** 2\n","\n","#     Denominator = (2 * mu_x * mu_y + C1) * (2 * Sigma_xy + C2)\n","#     #sigma_x^2 is the variance of x\n","#     Sigma_x_sq = func.conv2d(y_pred * y_pred, window, padding=0, groups=channel) - mu_x * mu_x\n","#     #sigma_y^2 is the variance of y\n","#     Sigma_y_sq = func.conv2d(y_true * y_true, window, padding=0, groups=channel) - mu_y * mu_y\n","\n","#     Numerator = (mu_x * mu_x + C1) *(Sigma_x_sq + Sigma_y_sq + C2)\n","\n","#     ssim_map = Denominator / Numerator\n","\n","#     if size_average:\n","#         return ssim_map.mean()\n","#     else:\n","#         return ssim_map.mean(1).mean(1).mean(1)\n","\n","# def depth_loss(y_pred, y_true):\n","#   #Ldepth(y, ŷ)\n","#   l_depth = l1_criterion(y_pred, y_true)\n","#   #Lgrad(y, ŷ)\n","#   dx_true, dy_true = image_gradients(y_true)\n","#   dx_pred, dy_pred = image_gradients(y_pred)\n","#   l_edges = torch.mean(torch.abs(dy_pred - dy_true) + torch.abs(dx_pred - dx_true))\n","#   #LSSIM (y, ŷ)\n","#   l_ssim = torch.clamp((1 - ssim(y_pred, y_true)) * 0.5, 0, 1)\n","\n","#   #loss \n","#   w1 = 0.1\n","#   w2 = 1.0\n","#   w3 = 1.0\n","#   loss= (w1 * l_depth) + (w2 * l_edges) + (w3 * l_ssim)\n","#   return loss"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yRau53IAfpoj"},"source":["model"]},{"cell_type":"code","metadata":{"id":"V_yVQeeHfqbe"},"source":["import torch\n","import torch.nn as nn\n","from torchvision import models\n","import torch.nn.functional as F\n","\n","class UpSample(nn.Sequential):\n","    def __init__(self, skip_input, output_features):\n","        super(UpSample, self).__init__()        \n","        self.convA = nn.Conv2d(skip_input, output_features, kernel_size=3, stride=1, padding=1)\n","        self.leakyreluA = nn.LeakyReLU(0.2)\n","        self.convB = nn.Conv2d(output_features, output_features, kernel_size=3, stride=1, padding=1)\n","        self.leakyreluB = nn.LeakyReLU(0.2)\n","\n","    def forward(self, x, concat_with):\n","        up_x = F.interpolate(x, size=[concat_with.size(2), concat_with.size(3)], mode='bilinear', align_corners=True)\n","        return self.leakyreluB( self.convB( self.leakyreluA(self.convA( torch.cat([up_x, concat_with], dim=1)  ) )))\n","        # return self.leakyreluB( self.convB( self.convA( torch.cat([up_x, concat_with], dim=1)  ) )  )\n","\n","class Decoder(nn.Module):\n","    def __init__(self, num_features=1664, decoder_width = 1.0):\n","        super(Decoder, self).__init__()\n","        features = int(num_features * decoder_width)\n","\n","        self.conv2 = nn.Conv2d(num_features, features, kernel_size=1, stride=1, padding=0)\n","\n","        self.up1 = UpSample(skip_input=features//1 + 256, output_features=features//2)\n","        self.up2 = UpSample(skip_input=features//2 + 128,  output_features=features//4)\n","        self.up3 = UpSample(skip_input=features//4 + 64,  output_features=features//8)\n","        self.up4 = UpSample(skip_input=features//8 + 64,  output_features=features//16)\n","\n","        self.conv3 = nn.Conv2d(features//16, 1, kernel_size=3, stride=1, padding=1)\n","\n","    def forward(self, features):\n","        x_block0, x_block1, x_block2, x_block3, x_block4 = features[3], features[4], features[6], features[8], features[12]\n","        x_d0 = self.conv2(F.relu(x_block4))\n","\n","        x_d1 = self.up1(x_d0, x_block3)\n","        x_d2 = self.up2(x_d1, x_block2)\n","        x_d3 = self.up3(x_d2, x_block1)\n","        x_d4 = self.up4(x_d3, x_block0)\n","        return self.conv3(x_d4)\n","\n","class Encoder(nn.Module):\n","    def __init__(self):\n","        super(Encoder, self).__init__()\n","        # pretrained (bool): If True, returns a model pre-trained on ImageNet\n","        self.original_model = models.densenet169( pretrained=True )\n","\n","    def forward(self, x):\n","        features = [x]\n","        for k, v in self.original_model.features._modules.items(): features.append( v(features[-1]) )\n","        return features\n","\n","class DepthModel(nn.Module):\n","    def __init__(self):\n","        super(DepthModel, self).__init__()\n","        self.encoder = Encoder()\n","        self.decoder = Decoder()\n","\n","    def forward(self, x):\n","        return self.decoder( self.encoder(x) )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"s17wBkmnft0h"},"source":["train"]},{"cell_type":"code","metadata":{"id":"jTMqk7CifubM","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["0b79a8fafc9c438883598ab807f9742e","b651f60f6d3a43659907e4eab45696ea","e792657230da4674a070b68eaf9a83a1","59950f5a17da4bff8d83650b349fd2a0","d5c6bb7a0e254454b68501d9cb2d826f","0d8449c52fe144179dab8be4d7925fac","c73018b4a13a4efb8e3dd5702f7dcbe1","5e22736ee103410a850f9f67b9803997"],"output_embedded_package_id":"13v6jzSMhbotTv-chqORRnIb8AUKghTNF"},"executionInfo":{"status":"ok","timestamp":1628000286270,"user_tz":-480,"elapsed":15161841,"user":{"displayName":"崔人文","photoUrl":"","userId":"05996306314003136969"}},"outputId":"588b4301-259a-4c33-9048-d51badaacd94"},"source":["import time\n","import torch\n","import torch.nn as nn\n","import matplotlib.pyplot as plt\n","import os\n","import cv2\n","\n","def main():\n","    # send the tensor to GPU if you have a GPU; otherwise, send the tensor to CPU\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    # Create model\n","    model = DepthModel().to(device)\n","    \n","    # set batch size and the numer of epoches\n","    batch_size = 8\n","    num_epoch = 50\n","    \n","    # Adam optimizer with learning rate of 0.0001\n","    optimizer = torch.optim.Adam( model.parameters(), lr=0.0001 )\n","\n","    # train\n","    annotated_directory = \"./targetdir/KITTI_small_data/kitti/annotated/train/\"\n","    inpainted_directory = \"./targetdir/KITTI_small_data/kitti/inpainted/train/\"\n","    files = []\n","    for directory in os.listdir(annotated_directory):\n","        if not directory.startswith('.'):\n","            for image in os.listdir(os.path.join(annotated_directory,directory,\"proj_depth/groundtruth/image_03\")):\n","                files.append((directory, image))\n","    train_set = KITTI_TrainAugmentDataset(files, inpainted_directory)\n","    train_loader = DataLoader(train_set, batch_size, shuffle=True)\n","    \n","    #val\n","    val_annotated_directory = \"./targetdir/KITTI_small_data/kitti/annotated/val/\"\n","    val_inpainted_directory = \"./targetdir/KITTI_small_data/kitti/inpainted/val/\"\n","    files = []\n","    for directory in os.listdir(val_annotated_directory):\n","        if not directory.startswith('.'):\n","            for image in os.listdir(os.path.join(val_annotated_directory,directory,\"proj_depth/groundtruth/image_03\")):\n","                if not image.startswith('.'):\n","                    files.append((directory, image))\n","    test_set = KITTI_TestDataset(files, val_inpainted_directory)\n","    test_loader = DataLoader(test_set, batch_size, shuffle=False)\n","\n","    train_avg_losses = []\n","    test_avg_losses = []\n","\n","    # Start training...\n","    for epoch in range(num_epoch):\n","        epoch_start_time = time.time()\n","        train_loss = 0.0\n","        test_loss = 0.0\n","        \n","        # Switch to train mode\n","        model.train()\n","        for i, sample_batched in enumerate(train_loader):\n","            # zero the gradients\n","            optimizer.zero_grad()\n","            # Prepare RGB sample and corresponding depth ground truth, and send only one batch to the device.\n","            train_image = torch.autograd.Variable(sample_batched['image'].to(device))\n","            train_depth = torch.autograd.Variable(sample_batched['depth'].to(device))\n","            # Predict depth\n","            train_output = model(train_image)\n","            # Compute the loss between the prediction and the ground truth\n","            batch_loss = depth_loss(train_output, train_depth)\n","            # auto-compute all gradients \n","            batch_loss.backward()\n","            # update the parameters of the model using the computed gradients\n","            optimizer.step()\n","            \n","            # accumulate loss\n","            train_loss += batch_loss.item()\n","            \n","            # display information about running speed and batch loss\n","            if i % 10 == 0:\n","                print('Epoch [{}/{}][{}/{}], {:.2f} sec(s), Batch loss:{:.5f} (Avg:{:.5f})'\n","                  .format(epoch+1, num_epoch, i+1, train_loader.__len__(), (time.time()-epoch_start_time)*10/(i+1), batch_loss, train_loss/(i+1)))\n","            if i % 400 == 0:\n","                input_rgb = train_image.permute(0, 2, 3, 1)\n","                input_rgb = torch.squeeze(input_rgb[-1]).cpu().detach().numpy()\n","                plt.figure(1)\n","                plt.imshow( input_rgb )\n","                plt.show()\n","                output_depth = train_output.permute(0, 2, 3, 1)\n","                output_depth = torch.squeeze(output_depth[-1]).cpu().detach().numpy()\n","                plt.figure(2)\n","                plt.imshow( -output_depth, cmap='plasma' )\n","                plt.show()\n","        \n","        \n","        # switch to test mode \n","        model.eval()\n","        # disable any gradient calculation\n","        with torch.no_grad():\n","            for i, sample_batched in enumerate(test_loader):\n","                # prepare test dataset\n","                test_image = torch.autograd.Variable(sample_batched['image'].to(device))\n","                test_depth = torch.autograd.Variable(sample_batched['depth'].to(device))\n","                # predict\n","                test_output = model(test_image)\n","                # loss\n","                batch_loss = depth_loss(test_output, test_depth)\n","                \n","                # accumulate loss\n","                test_loss += batch_loss.item()\n","                \n","                \n","                # display the depth prediction of last RGB image in test dataset\n","                if i == len(test_loader)-1:\n","                    # change the dimensions of the output tensor from (N x C x H x W) to (N x H x W x C)\n","                    output_depth = test_output.permute(0, 2, 3, 1)\n","                    # removes all dimensions with a length of one from tensor, it will return a tensor with the size of (H x W)\n","                    # transfer from tensor to numpy after removing gradients using torch.detach()\n","                    # output_depth = torch.squeeze(output_depth[-1]).detach().numpy()\n","                    output_depth = torch.squeeze(output_depth[-1]).cpu().detach().numpy()\n","                    plt.figure(1)\n","                    plt.imshow( output_depth, cmap='plasma' )\n","                    plt.show()\n","            \n","            # record average batch losses for training and test sets at one epoch\n","            train_avg_losses.append(train_loss/train_loader.__len__())\n","            test_avg_losses.append(test_loss/test_loader.__len__())\n","            # display information about running speed of one epoch and batch loss\n","            print('Epoch [{}/{}], {:.2f} sec(s), Avg Train loss:{:.5f}, Avg Test loss:{:.5f}'\n","                  .format(epoch+1, num_epoch, time.time()-epoch_start_time, train_loss/train_loader.__len__(), test_loss/test_loader.__len__()))\n","            \n","\n","    # plot average batch losses for training and test sets\n","    plt.figure(3)\n","    plt.plot(train_avg_losses, 'o-', label='average train loss')\n","    plt.plot(test_avg_losses, 'o-', label='average test loss')\n","    plt.legend()\n","    plt.title('train/test losses')\n","    plt.savefig('losses.png')\n","    plt.show()\n","    \n","\n","    # save model's parameters\n","    path = 'nyusmall_para.pt'\n","    torch.save(model.state_dict(), path)\n","\n","if __name__ == '__main__':\n","    main()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]}]}